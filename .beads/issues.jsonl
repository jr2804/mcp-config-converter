{"id":"mcp-18p","title":"Rename of CLI parameter \"--llm-provider-type\"","description":"\"--llm-provider-type\" is too long and originates from an earlier version, where we had two different values for llm-provider and llm-provider-type. Actions:\n\n- The parameter should be renamed/shortened to \"--llm-provider\"\n- \"--llm-provider-type\" may still remain as an alias, for legacy compatibilty.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T03:22:06Z","updated_at":"2026-01-09T09:34:10.0644248+01:00","closed_at":"2026-01-04T17:36:38.1593506+01:00","close_reason":"Renamed --llm-provider-type to --llm-provider with --llm-provider-type as alias for backward compatibility"}
{"id":"mcp-2v7","title":"Default output paths should be configurable via environment variable","description":"The default output paths per output format/provider are currently hard-coded. They may be overridden by CLI parameter, but it might be useful to have environment variables that define the defaults. They should be named like e.g., \"MCP_CONFIG_CONV_[target-provider]_DEFAULT_OUTPUT.\n\nThe result table of the CLI command \"uv run mcp-config-converter show-defaults\" should thus also be adapted:\n- add a new column for showing the environment variable name\n- add another column that shows override values from these environment variables","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T03:20:14Z","updated_at":"2026-01-09T09:34:10.0793102+01:00","closed_at":"2026-01-04T19:40:01.669828+01:00","close_reason":"Both issues have been fully implemented. See git log for details."}
{"id":"mcp-41x","title":"Add just command to actually run the conversion","description":"Add a new justfile command \"convert\", which has one argument \"target\". This command will convert the \"main\" MCP server config located under @.vscode/mcp.json to a desired format (opencode, gemini, etc.). The commands will also have a varargs argument to provide additional LLM-provider info.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T03:13:56Z","updated_at":"2026-01-09T09:34:10.0781475+01:00","closed_at":"2026-01-04T04:49:06.8274236+01:00","close_reason":"Closed"}
{"id":"mcp-619","title":"Improve column \"API Key Source\"","description":"When calling the llm-check CLI command, the column \"API Key Source\" unnecessarily shows \"Missing (..)\" and makes the row longer than necessary. Instead, we will color-code if the environment variable is set (green) or not set (yellow).","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-04T19:36:55.7499344+01:00","updated_at":"2026-01-04T23:32:47.4598132+01:00","closed_at":"2026-01-04T23:32:47.4598132+01:00","close_reason":"Already completed in commit 6e52d0f. Changed API Key Source column from showing 'Missing (...)' to color-coded status: [green]Configured[/green] for configured, [yellow]Missing[/yellow] for not configured, N/A (Local) for local providers."}
{"id":"mcp-635","title":"Add authentication verification to llm-check command","description":"## Context\n\nThe `llm-check` command in `mcp_config_converter/cli/llm_check.py` displays provider status in a table (lines 65-131). Currently, \"Status\" column only checks if an API key is configured in environment variables, but does not verify if API key is valid/authenticated.\n\n## Problem\n\nCurrent status logic:\n\n1. Line 108: \"[green]✓ Available[/green]\" - only checks if provider is configured (API key exists)\n2. Line 118: \"[yellow]⚠ Not Configured[/yellow]\" - API key is missing\n3. Line 115: \"[yellow]⚠ Not Available[/yellow]\" - Provider without API key (e.g., ollama)\n\nThe issue is that \"Available\" status does not verify if API key can successfully authenticate with the provider. Invalid or expired keys will show as \"Available\" but will fail when actually used.\n\n## Implementation Details\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/set_keys#check_valid_key):\n\n```python\nfrom litellm import check_valid_key\n\n# Check if API key is valid for a model\nresponse = check_valid_key(model=\"gpt-3.5-turbo\", api_key=key)\n# Returns: True if valid, False if invalid\n```\n\n## Requirements\n\n1. Import `check_valid_key` from `litellm` in llm_check.py\n\n2. Update status logic for configured providers:\n   - If API key exists AND `check_valid_key()` returns True: \"[green]✓ Available[/green]\"\n   - If API key exists BUT `check_valid_key()` returns False: \"[red]✗ Not allowed[/red]\"\n   - If API key does not exist: \"[yellow]⚠ Not Configured[/yellow]\"\n\n3. For providers without API keys (e.g., ollama):\n   - Keep \"[yellow]⚠ Not Available[/yellow]\" or \"N/A (Local)\" status\n\n4. Error handling:\n   - Handle network errors, rate limits, timeouts\n   - Add optional `--no-auth-check` flag to skip authentication\n   - Consider timeout and retry logic for auth checks\n   - Log warnings when auth check fails for non-key-related reasons\n\n5. Performance considerations:\n   - Auth checks can be slow (requires API calls)\n   - Consider parallel checking for multiple providers\n   - Cache auth results during command execution\n   - Add progress indicator for slow checks\n\n## Files to Modify\n\n- `mcp_config_converter/cli/llm_check.py` - Update status checking logic in llm_check() function\n\n## Additional Considerations\n\n- Use provider's default model for authentication check (e.g., openai -\u003e gpt-4o-mini)\n- Some providers may not support `check_valid_key()` - handle gracefully\n- Consider adding verbose mode to show authentication error details\n- Update help text/documentation to reflect new authentication checking\n\n## Testing Considerations\n\n- Test with valid API keys\n- Test with invalid/expired API keys\n- Test with missing API keys\n- Test with providers that don't require auth (ollama)\n- Test network error scenarios\n- Verify performance with multiple providers","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-04T03:44:21.0848202+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T23:25:38.5261463+01:00","closed_at":"2026-01-04T23:25:38.5261463+01:00","close_reason":"Authentication verification already fully implemented. check_provider_auth() function uses check_valid_key() from litellm to verify API keys. Status logic already handles: Valid (green), Not allowed (red), Not Configured (yellow), N/A (Local) for ollama. --no-auth-check flag already supported. Error handling for network errors, rate limits, timeouts already in place. Caching of auth results via auth_cache dict prevents redundant API calls."}
{"id":"mcp-7in","title":"Improve log/console output during conversion","description":"If the output is written to an output file, the console should indicate chosen/selected provider, model and possibly other configured parameters","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-05T01:02:36.2560707+01:00","created_by":"Jan Reimes","updated_at":"2026-01-05T01:02:36.2560707+01:00"}
{"id":"mcp-fw2","title":"Drop support for deprecated env. var. MCP_CONFIG_CONF_LLM_PROVIDER_TYPE and CLI flag --llm-provider-type","description":"Keep only support for MCP_CONFIG_CONF_LLM_PROVIDER and CLI flag --llm-provider. Also update documentation/README.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-04T18:34:23.0680675+01:00","updated_at":"2026-01-04T19:40:01.6887927+01:00","closed_at":"2026-01-04T19:40:01.6887927+01:00","close_reason":"Both issues have been fully implemented. See git log for details."}
{"id":"mcp-j7t","title":"Remove interactive mode","description":"interactive mode is too complicated and is actually harder to use than default CLI usage.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-05T00:49:44.566665+01:00","created_by":"Jan Reimes","updated_at":"2026-01-05T00:49:44.566665+01:00"}
{"id":"mcp-llx","title":"Remove redundancy: --preferred-provider/-pp vs --llm-provider-type","description":"With the new version, an explicit preferred LLM provider is not necessary. auto-selection is always used when no --llm-provider-type is configured. Thus, --preferred-provider/-pp can be removed.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T17:27:15.5530349+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T17:33:38.5281587+01:00","closed_at":"2026-01-04T17:33:38.5281587+01:00","close_reason":"Removed redundant --preferred-provider/-pp option as requested"}
{"id":"mcp-ps3","title":"Fix LiteLLMClient.get_available_models() to use get_valid_models()","description":"## Context\n\nThe `LiteLLMClient.get_available_models()` method in `mcp_config_converter/llm/client.py` (lines 180-198) currently uses `model_cost` dictionary to retrieve available models. This is incorrect because model_cost is a pricing/metadata dictionary, not a comprehensive list of available models.\n\n## Problem\n\nCurrent implementation issues:\n\n1. Uses `model_cost` dictionary which only contains pricing information\n2. Returns incorrect/filtered model lists for providers\n3. Does not leverage LiteLLM's built-in `get_valid_models()` helper function\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/set_keys#get_valid_models), correct approach is to use `get_valid_models()` helper function.\n\n## Implementation Details\n\nFrom LiteLLM docs:\n\n```python\nfrom litellm import get_valid_models\n\n# Basic usage - reads .env and returns supported models\nvalid_models = get_valid_models()\n\n# Query provider endpoints for valid models (more accurate but slower)\nvalid_models = get_valid_models(check_provider_endpoint=True)\n\n# Check specific provider only\nvalid_models = get_valid_models(\n    check_provider_endpoint=True, \n    custom_llm_provider=\"openai\"\n)\n```\n\n## Requirements\n\n1. Replace `get_available_models()` implementation:\n   - Import `get_valid_models` from `litellm`\n   - Call `get_valid_models()` instead of accessing `model_cost`\n   - Filter results by provider if `self.provider` is set\n\n2. Consider using `check_provider_endpoint=True`:\n   - More accurate model lists by querying provider endpoints\n   - Currently supported providers: OpenAI, Fireworks AI, LiteLLM Proxy, Gemini, XAI, Anthropic\n   - Trade-off: slower but more accurate\n\n3. Handle errors gracefully:\n   - `get_valid_models()` can raise exceptions\n   - Log errors and return empty list on failure\n   - Maintain current error handling pattern\n\n## Files to Modify\n\n- `mcp_config_converter/llm/client.py` - Replace `get_available_models()` method implementation\n\n## Testing Considerations\n\n- Test with various providers (openai, anthropic, gemini, ollama, etc.)\n- Verify model filtering by provider works correctly\n- Test with and without `check_provider_endpoint=True`\n- Ensure error handling works when provider endpoints are unavailable\n- Test with environment variables set for different providers","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-04T03:44:21.1073251+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T23:23:10.320893+01:00","closed_at":"2026-01-04T23:23:10.320893+01:00","close_reason":"Added check_provider_endpoint parameter to LiteLLMClient to support querying provider endpoints for more accurate model lists. Updated get_available_models() to use get_valid_models(check_provider_endpoint=True) when enabled. Added environment variable MCP_CONFIG_CONF_LLM_CHECK_PROVIDER_ENDPOINT and CLI argument --check-provider-endpoint for configuration."}
{"id":"mcp-tk5","title":"Apply pretty formatting of JSON/TOML/YAML output files via jsonfmt","description":"Use Python jsonfmt library to pretty print output files, see: https://github.com/seamile/jsonfmt (install via: uv add jsonfmt)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-08T01:04:08.6590126+01:00","created_by":"Jan Reimes","updated_at":"2026-01-08T01:04:08.6590126+01:00"}
{"id":"mcp-uka","title":"Add short code '-lm' for CLI as an alias to --llm-model","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-05T00:47:16.7912347+01:00","created_by":"Jan Reimes","updated_at":"2026-01-05T00:47:16.7912347+01:00"}
{"id":"mcp-w4d","title":"Support for ISON format for sending input config to LLM","description":"In addition to the already existing TOON format, we should also support ISON format, see: https://github.com/maheshvaikri-code/ison and https://www.ison.dev/getting-started.html. The necessary Python packages ison-py and isonantic are already installed. The current bool CLI switch for encode-toon should thus be turned into a general encoding parameter (ison, toon, none)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-05T00:26:19.291009+01:00","created_by":"Jan Reimes","updated_at":"2026-01-05T00:26:19.291009+01:00"}
{"id":"mcp-zqv","title":"Enable disk caching for LiteLLM completion calls","description":"## Context\n\nThe project already has `litellm[caching]\u003e=1.59.7` dependency installed in pyproject.toml (line 15), which provides disk caching capabilities. However, caching is not currently enabled or configured in LiteLLMClient class.\n\n## Problem\n\nCaching can significantly reduce costs and improve performance by:\n- Avoiding repeated API calls for identical prompts\n- Reducing latency for repeated operations\n- Lowering token usage and costs during development/testing\n\n## Implementation Details\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/caching/all_caches), disk caching is enabled by:\n\n1. Import Cache class:\n   ```python\n   from litellm.caching.caching import Cache\n   ```\n\n2. Initialize cache globally or per-client:\n   ```python\n   litellm.cache = Cache(type=\"disk\", disk_cache_dir=None)\n   # or using context manager\n   litellm.enable_cache(type=\"disk\")\n   ```\n\n3. Pass caching=True to completion calls:\n   ```python\n   response = completion(model=\"...\", messages=[...], caching=True)\n   ```\n\n## Requirements\n\n1. Modify `LiteLLMClient.__init__()` to:\n   - Add optional parameters: `enable_cache` (bool, default=False), `cache_type` (str, default=\"disk\"), `cache_dir` (str|None)\n   - Initialize litellm cache if enable_cache is True\n   - Support environment variable `MCP_CONFIG_CONF_LLM_CACHE_ENABLED` for default behavior\n\n2. Modify `LiteLLMClient.generate()` method:\n   - Pass `caching=True` to completion() calls when cache is enabled\n   - Consider adding cache control parameters (no-cache, no-store, ttl, s-maxage) as kwargs\n\n3. Add to constants.py:\n   - Environment variable constant: `MCP_CONFIG_CONF_LLM_CACHE_ENABLED`\n\n4. Add to CLI arguments (optional):\n   - Consider adding --enable-cache flag to relevant commands\n\n## Files to Modify\n\n- `mcp_config_converter/llm/client.py` - Main implementation in LiteLLMClient class\n- `mcp_config_converter/cli/constants.py` - Add cache environment variable constant\n- `mcp_config_converter/cli/arguments.py` - Optional: add CLI arguments for cache control\n\n## Testing Considerations\n\n- Test cache hit behavior with identical prompts\n- Test cache miss behavior with different prompts\n- Verify cache files are created in expected location\n- Test cache disable functionality\n- Ensure backward compatibility when cache is not enabled","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-04T03:44:21.0759305+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T23:25:09.1566252+01:00","closed_at":"2026-01-04T23:25:09.1566252+01:00","close_reason":"Disk caching feature already fully implemented in commit 1d2e323. All requirements satisfied: enable_cache parameter in __init__(), cache initialization in __init__(), caching=True passed to completion() calls, environment variable MCP_CONFIG_CONF_LLM_CACHE_ENABLED supported, CLI arguments --enable-cache/-ec and --cache-dir added to convert and llm_check commands. All cache tests passing."}
