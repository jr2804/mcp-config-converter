# LLM Provider API Keys Configuration
#
# This file contains environment variable names for all supported LLM providers.
# Copy this file to .env and fill in your actual API keys.
#
# NOTE: Never commit your actual .env file with real API keys!
#
# With the unified LiteLLM implementation, all providers are now supported through
# a single interface. Simply set the appropriate API key for your chosen provider.

# MCP Config Converter CLI Configuration
# These environment variables configure a complete LiteLLM client instance.
# CLI arguments will override these environment variables.
#
# MCP_CONFIG_CONF_LLM_BASE_URL: Custom base URL for LiteLLM API endpoint
# MCP_CONFIG_CONF_LLM_PROVIDER: LiteLLM client type (e.g., 'openai', 'anthropic', 'gemini')
#                                     See: https://docs.litellm.ai/docs/providers
#                                     Note: This is NOT the litellm proxy mode
# MCP_CONFIG_CONF_LLM_MODEL: Model identifier (e.g., 'gpt-4', 'claude-3-5-sonnet-20241022')
# MCP_CONFIG_CONF_API_KEY: Generic API key (can be used instead of provider-specific keys)
# MCP_CONFIG_CONF_PREFERRED_PROVIDER: Preferred provider for auto-selection (default: 'auto')
#
# MCP_CONFIG_CONF_LLM_BASE_URL=your_custom_base_url
# MCP_CONFIG_CONF_LLM_PROVIDER=openai
# MCP_CONFIG_CONF_LLM_MODEL=gpt-4
# MCP_CONFIG_CONF_API_KEY=your_api_key
# MCP_CONFIG_CONF_PREFERRED_PROVIDER=auto

# MCP_CONFIG_CONF_LLM_CACHE_ENABLED: Enable disk caching for LLM API calls ("true" or "false", default: "false")
# MCP_CONFIG_CONF_LLM_CACHE_ENABLED=false

# ===== Test Configuration =====
# These environment variables control the test suite behavior when running pytest.
# See tests/test_cli.py for implementation details.
#
# MCP_CONFIG_CONF_MAX_TESTS: Maximum number of LLM provider tests to run per output provider (default: 2)
#                             This limits test execution time when testing multiple providers.
# MCP_CONFIG_CONF_TEST_LLM_PROVIDERS: Comma/semicolon/colon-separated list of LLM providers to test.
#                                      Format: "provider/model" (e.g., "ollama/gemma3, deepseek/deepseek-chat")
#                                      If not set, defaults to "ollama/-1" (no API key required).
#                                      Supported separators: comma (,) or semicolon (;) (plus blanks)
#                                      Mixed separators are also supported.
#
# Examples:
# MCP_CONFIG_CONF_MAX_TESTS=4
# MCP_CONFIG_CONF_TEST_LLM_PROVIDERS=ollama/gemma3, deepseek/deepseek-chat
# MCP_CONFIG_CONF_TEST_LLM_PROVIDERS=ollama/gemma3; deepseek/deepseek-chat,sambanova/model
#
# Example entries with the two selected test environment variables:
# MCP_CONFIG_CONF_MAX_TESTS=2
# MCP_CONFIG_CONF_TEST_LLM_PROVIDERS=ollama/-1

# ===== Default Output Path Configuration =====
# These environment variables allow overriding default output paths per provider
#
# MCP_CONFIG_CONV_VSCODE_DEFAULT_OUTPUT: Default output path for VS Code format (e.g., ".vscode/custom-mcp.json")
# MCP_CONFIG_CONV_GEMINI_DEFAULT_OUTPUT: Default output path for Gemini format (e.g., ".gemini/custom-mcp.json")
# MCP_CONFIG_CONV_CLAUDE_DEFAULT_OUTPUT: Default output path for Claude format (e.g., ".claude/custom-mcp.json")
# MCP_CONFIG_CONV_CODEX_DEFAULT_OUTPUT: Default output path for Codex format (e.g., ".codex/custom-mcp.json")
# MCP_CONFIG_CONV_OPENCODE_DEFAULT_OUTPUT: Default output path for OpenCode format (e.g., ".opencode/custom-mcp.json")
# MCP_CONFIG_CONV_MISTRAL_DEFAULT_OUTPUT: Default output path for Mistral format (e.g., ".mistral/custom-mcp.json")
# MCP_CONFIG_CONV_QWEN_DEFAULT_OUTPUT: Default output path for Qwen format (e.g., ".qwen/custom-mcp.json")
# MCP_CONFIG_CONV_LLXPRT_DEFAULT_OUTPUT: Default output path for LLXPRT format (e.g., ".llxprt/custom-mcp.json")
# MCP_CONFIG_CONV_CRUSH_DEFAULT_OUTPUT: Default output path for Crush format (e.g., ".crush/custom-mcp.json")
# MCP_CONFIG_CONV_DEFAULT_OUTPUT: Generic default output path for all providers (e.g., "custom-mcp.json")
#
# Use the "show-defaults" command to see current values and environment variable overrides

# ===== LiteLLM Supported Provider API Keys =====
# Set the API key for the provider you want to use with LiteLLM

# OpenAI API Key (for GPT models)
# OPENAI_API_KEY=your_openai_api_key_here

# Anthropic API Key (for Claude models)
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google API Keys (for Gemini models)
# GOOGLE_API_KEY=your_google_api_key_here
# GEMINI_API_KEY=your_gemini_api_key_here
# GOOGLE_GENERATIVE_AI_API_KEY=your_google_generative_ai_api_key_here

# Mistral API Key
# MISTRAL_API_KEY=your_mistral_api_key_here

# DeepSeek API Key
# DEEPSEEK_API_KEY=your_deepseek_api_key_here

# OpenRouter API Key
# OPENROUTER_API_KEY=your_openrouter_api_key_here

# Perplexity API Key
# PERPLEXITY_API_KEY=your_perplexity_api_key_here

# SambaNova API Key
# SAMBANOVA_API_KEY=your_sambanova_api_key_here

# ZAI API Key
# ZAI_API_KEY=your_zai_api_key_here

# Ollama - typically doesn't require an API key for local instances
# OLLAMA_BASE_URL=http://localhost:11434

# ===== Legacy Provider-Specific API Keys =====
# These are maintained for backward compatibility with legacy provider implementations
# You should use the LiteLLM provider instead (see above)


# ===== ChunkHound MCP Server Configuration =====
# ChunkHound is an MCP server for semantic code search and analysis
# See: https://chunkhound.github.io/configuration/

# Main Configuration
# CHUNKHOUND_DEBUG=true
# CHUNKHOUND_CONFIG_FILE=/path/to/.chunkhound.json

# Database Configuration
# CHUNKHOUND_DATABASE__PROVIDER=duckdb
# CHUNKHOUND_DATABASE__PATH=/custom/db/path

# Embedding Configuration (for semantic search)
# CHUNKHOUND_EMBEDDING__PROVIDER=openai
# CHUNKHOUND_EMBEDDING__BASE_URL=http://localhost:11434/v1
# CHUNKHOUND_EMBEDDING__API_KEY=pa-your-voyage-key
# CHUNKHOUND_EMBEDDING__MODEL=qwen3-embedding:4b
# other models to consider:
# - mxbai-embed-large:latest
# - embeddinggemma:latest
# - granite-embedding:278m


# CHUNKHOUND_EMBEDDING__RERANK_MODEL=rerank-lite-1

# Indexing Configuration (comma-separated patterns)
# CHUNKHOUND_INDEXING__INCLUDE="**/*.py,**/*.js,**/*.ts"
# CHUNKHOUND_INDEXING__EXCLUDE="**/node_modules/**,**/__pycache__/**"

# LLM Configuration (for code research features)
# CHUNKHOUND_LLM_PROVIDER=ollama
# CHUNKHOUND_LLM_API_KEY=your_api_key
# CHUNKHOUND_LLM_BASE_URL=https://api.openai.com/v1

# CHUNKHOUND_LLM_UTILITY_MODEL=functiongemma:latest
# CHUNKHOUND_LLM_SYNTHESIS_MODEL=granite4:latest

# other models to consider:
# - gemma3:latest (utility & synthesis)
# - functiongemma:latest (utility)
# - ministral-3:3b (synthesis)
# - granite4:latest (utility & synthesis)

# misc. settings
# CHUNKHOUND_LLM_UTILITY_PROVIDER=openai
# CHUNKHOUND_LLM_SYNTHESIS_PROVIDER=claude-code-cli
# CHUNKHOUND_LLM_CODEX_REASONING_EFFORT=medium
# CHUNKHOUND_LLM_TIMEOUT=60
# CHUNKHOUND_LLM_MAX_RETRIES=3

# MCP Server Configuration (HTTP mode only - stdio is default)
# CHUNKHOUND_MCP__HOST=localhost
# CHUNKHOUND_MCP__PORT=8080
