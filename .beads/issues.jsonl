{"id":"mcp-635","title":"Add authentication verification to llm-check command","description":"## Context\n\nThe  command in  displays provider status in a table (lines 65-131). Currently, the \"Status\" column only checks if an API key is configured in environment variables, but does not verify if the API key is valid/authenticated.\n\n## Problem\n\nCurrent status logic:\n\n1. Line 108: \"[green]✓ Available[/green]\" - only checks if provider is configured (API key exists)\n2. Line 118: \"[yellow]⚠ Not Configured[/yellow]\" - API key is missing\n3. Line 115: \"[yellow]⚠ Not Available[/yellow]\" - Provider without API key (e.g., ollama)\n\nThe issue is that \"Available\" status does not verify if the API key can successfully authenticate with the provider. Invalid or expired keys will show as \"Available\" but will fail when actually used.\n\n## Implementation Details\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/set_keys#check_valid_key):\n\nCtrl click to launch VS Code Native REPL\n\n## Requirements\n\n1. Import  from  in llm_check.py\n\n2. Update status logic for configured providers:\n   - If API key exists AND  returns True: \"[green]✓ Available[/green]\"\n   - If API key exists BUT  returns False: \"[red]✗ Not allowed[/red]\"\n   - If API key does not exist: \"[yellow]⚠ Not Configured[/yellow]\"\n\n3. For providers without API keys (e.g., ollama):\n   - Keep \"[yellow]⚠ Not Available[/yellow]\" or \"N/A (Local)\" status\n\n4. Error handling:\n   - Handle network errors, rate limits, timeouts\n   - Add optional  flag to skip authentication\n   - Consider timeout and retry logic for auth checks\n   - Log warnings when auth check fails for non-key-related reasons\n\n5. Performance considerations:\n   - Auth checks can be slow (requires API calls)\n   - Consider parallel checking for multiple providers\n   - Cache auth results during command execution\n   - Add progress indicator for slow checks\n\n## Files to Modify\n\n-  - Update status checking logic in llm_check() function\n\n## Additional Considerations\n\n- Use provider's default model for authentication check (e.g., openai -\u003e gpt-4o-mini)\n- Some providers may not support  - handle gracefully\n- Consider adding verbose mode to show authentication error details\n- Update help text/documentation to reflect new authentication checking\n\n## Testing Considerations\n\n- Test with valid API keys\n- Test with invalid/expired API keys\n- Test with missing API keys\n- Test with providers that don't require auth (ollama)\n- Test network error scenarios\n- Verify performance with multiple providers","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-04T03:44:21.0848202+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T03:50:11.3977595+01:00"}
{"id":"mcp-ps3","title":"Fix LiteLLMClient.get_available_models() to use get_valid_models()","description":"## Context\n\nThe  method in  (lines 180-198) currently uses the  dictionary to retrieve available models. This is incorrect because model_cost is a pricing/metadata dictionary, not a comprehensive list of available models.\n\n## Problem\n\nCurrent implementation issues:\n\n1. Uses  dictionary which only contains pricing information\n2. Returns incorrect/filtered model lists for providers\n3. Does not leverage LiteLLM's built-in  helper function\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/set_keys#get_valid_models), the correct approach is to use the  helper function.\n\n## Implementation Details\n\nFrom LiteLLM docs:\n\nCtrl click to launch VS Code Native REPL\n\n## Requirements\n\n1. Replace  implementation:\n   - Import  from \n   - Call  instead of accessing \n   - Filter results by provider if  is set\n\n2. Consider using :\n   - More accurate model lists by querying provider endpoints\n   - Currently supported providers: OpenAI, Fireworks AI, LiteLLM Proxy, Gemini, XAI, Anthropic\n   - Trade-off: slower but more accurate\n\n3. Handle errors gracefully:\n   -  can raise exceptions\n   - Log errors and return empty list on failure\n   - Maintain current error handling pattern\n\n## Files to Modify\n\n-  - Replace  method implementation\n\n## Testing Considerations\n\n- Test with various providers (openai, anthropic, gemini, ollama, etc.)\n- Verify model filtering by provider works correctly\n- Test with and without \n- Ensure error handling works when provider endpoints are unavailable\n- Test with environment variables set for different providers","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-04T03:44:21.1073251+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T03:50:08.2677942+01:00"}
{"id":"mcp-zqv","title":"Enable disk caching for LiteLLM completion calls","description":"## Context\n\nThe project already has the  dependency installed in pyproject.toml (line 15), which provides disk caching capabilities. However, caching is not currently enabled or configured in the LiteLLMClient class.\n\n## Problem\n\nCaching can significantly reduce costs and improve performance by:\n- Avoiding repeated API calls for identical prompts\n- Reducing latency for repeated operations\n- Lowering token usage and costs during development/testing\n\n## Implementation Details\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/caching/all_caches), disk caching is enabled by:\n\n1. Import Cache class:\n   Ctrl click to launch VS Code Native REPL\n\n2. Initialize cache globally or per-client:\n   Ctrl click to launch VS Code Native REPL\n\n3. Pass caching=True to completion calls:\n   Ctrl click to launch VS Code Native REPL\n\n## Requirements\n\n1. Modify  to:\n   - Add optional parameters:  (bool, default=False),  (str, default=\"disk\"),  (str|None)\n   - Initialize litellm cache if enable_cache is True\n   - Support environment variable  for default behavior\n\n2. Modify  method:\n   - Pass  to completion() calls when cache is enabled\n   - Consider adding cache control parameters (no-cache, no-store, ttl, s-maxage) as kwargs\n\n3. Add to constants.py:\n   - Environment variable constant: \n\n4. Add to CLI arguments (optional):\n   - Consider adding --enable-cache flag to relevant commands\n\n## Files to Modify\n\n-  - Main implementation in LiteLLMClient class\n-  - Add cache environment variable constant\n-  - Optional: add CLI arguments for cache control\n\n## Testing Considerations\n\n- Test cache hit behavior with identical prompts\n- Test cache miss behavior with different prompts\n- Verify cache files are created in expected location\n- Test cache disable functionality\n- Ensure backward compatibility when cache is not enabled","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-04T03:44:21.0759305+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T03:47:26.6149053+01:00"}
