{"id":"9de75e18-e5bd-472b-ae92-d9d05ba5655f","title":"Rename of CLI parameter \"--llm-provider-type\"","description":"\"--llm-provider-type\" is too long and originates from an earlier version, where we had two different values for llm-provider and llm-provider-type. Actions:\n\n- The parameter should be renamed/shortened to \"--llm-provider\"\n- \"--llm-provider-type\" may still remain as an alias, for legacy compatibilty.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T03:22:06Z","updated_at":"2026-01-04T03:22:06Z"}
{"id":"9f2a05fd-bd2f-4418-a384-5cb0334d44b2","title":"Add just command to actually run the conversion","description":"Add a new justfile command \"convert\", which has one argument \"target\". This command will convert the \"main\" MCP server config located under @.vscode/mcp.json to a desired format (opencode, gemini, etc.). The commands will also have a varargs argument to provide additional LLM-provider info.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T03:13:56Z","updated_at":"2026-01-04T03:13:56Z"}
{"id":"ef4354cc-7071-4016-9bee-fec170c68189","title":"Default output paths should be configurable via environment variable","description":"The default output paths per output format/provider are currently hard-coded. They may be overridden by CLI parameter, but it might be useful to have environment variables that define the defaults. They should be named like e.g., \"MCP_CONFIG_CONV_\u003ctarget-provider\u003e_DEFAULT_OUTPUT\n\nThe result table of the CLI command \"uv run mcp-config-converter show-defaults\" should thus also be adapted:\n- add a new column for showing the environment variable name\n- add another column that shows override values from these environment variables","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T03:20:14Z","updated_at":"2026-01-04T03:20:14Z"}
{"id":"mcp-635","title":"Add authentication verification to llm-check command","description":"## Context\n\nThe `llm-check` command in `mcp_config_converter/cli/llm_check.py` displays provider status in a table (lines 65-131). Currently, \"Status\" column only checks if an API key is configured in environment variables, but does not verify if API key is valid/authenticated.\n\n## Problem\n\nCurrent status logic:\n\n1. Line 108: \"[green]✓ Available[/green]\" - only checks if provider is configured (API key exists)\n2. Line 118: \"[yellow]⚠ Not Configured[/yellow]\" - API key is missing\n3. Line 115: \"[yellow]⚠ Not Available[/yellow]\" - Provider without API key (e.g., ollama)\n\nThe issue is that \"Available\" status does not verify if API key can successfully authenticate with the provider. Invalid or expired keys will show as \"Available\" but will fail when actually used.\n\n## Implementation Details\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/set_keys#check_valid_key):\n\n```python\nfrom litellm import check_valid_key\n\n# Check if API key is valid for a model\nresponse = check_valid_key(model=\"gpt-3.5-turbo\", api_key=key)\n# Returns: True if valid, False if invalid\n```\n\n## Requirements\n\n1. Import `check_valid_key` from `litellm` in llm_check.py\n\n2. Update status logic for configured providers:\n   - If API key exists AND `check_valid_key()` returns True: \"[green]✓ Available[/green]\"\n   - If API key exists BUT `check_valid_key()` returns False: \"[red]✗ Not allowed[/red]\"\n   - If API key does not exist: \"[yellow]⚠ Not Configured[/yellow]\"\n\n3. For providers without API keys (e.g., ollama):\n   - Keep \"[yellow]⚠ Not Available[/yellow]\" or \"N/A (Local)\" status\n\n4. Error handling:\n   - Handle network errors, rate limits, timeouts\n   - Add optional `--no-auth-check` flag to skip authentication\n   - Consider timeout and retry logic for auth checks\n   - Log warnings when auth check fails for non-key-related reasons\n\n5. Performance considerations:\n   - Auth checks can be slow (requires API calls)\n   - Consider parallel checking for multiple providers\n   - Cache auth results during command execution\n   - Add progress indicator for slow checks\n\n## Files to Modify\n\n- `mcp_config_converter/cli/llm_check.py` - Update status checking logic in llm_check() function\n\n## Additional Considerations\n\n- Use provider's default model for authentication check (e.g., openai -\u003e gpt-4o-mini)\n- Some providers may not support `check_valid_key()` - handle gracefully\n- Consider adding verbose mode to show authentication error details\n- Update help text/documentation to reflect new authentication checking\n\n## Testing Considerations\n\n- Test with valid API keys\n- Test with invalid/expired API keys\n- Test with missing API keys\n- Test with providers that don't require auth (ollama)\n- Test network error scenarios\n- Verify performance with multiple providers","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-04T03:44:21.0848202+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T04:07:42.4075474+01:00"}
{"id":"mcp-ps3","title":"Fix LiteLLMClient.get_available_models() to use get_valid_models()","description":"## Context\n\nThe `LiteLLMClient.get_available_models()` method in `mcp_config_converter/llm/client.py` (lines 180-198) currently uses `model_cost` dictionary to retrieve available models. This is incorrect because model_cost is a pricing/metadata dictionary, not a comprehensive list of available models.\n\n## Problem\n\nCurrent implementation issues:\n\n1. Uses `model_cost` dictionary which only contains pricing information\n2. Returns incorrect/filtered model lists for providers\n3. Does not leverage LiteLLM's built-in `get_valid_models()` helper function\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/set_keys#get_valid_models), correct approach is to use `get_valid_models()` helper function.\n\n## Implementation Details\n\nFrom LiteLLM docs:\n\n```python\nfrom litellm import get_valid_models\n\n# Basic usage - reads .env and returns supported models\nvalid_models = get_valid_models()\n\n# Query provider endpoints for valid models (more accurate but slower)\nvalid_models = get_valid_models(check_provider_endpoint=True)\n\n# Check specific provider only\nvalid_models = get_valid_models(\n    check_provider_endpoint=True, \n    custom_llm_provider=\"openai\"\n)\n```\n\n## Requirements\n\n1. Replace `get_available_models()` implementation:\n   - Import `get_valid_models` from `litellm`\n   - Call `get_valid_models()` instead of accessing `model_cost`\n   - Filter results by provider if `self.provider` is set\n\n2. Consider using `check_provider_endpoint=True`:\n   - More accurate model lists by querying provider endpoints\n   - Currently supported providers: OpenAI, Fireworks AI, LiteLLM Proxy, Gemini, XAI, Anthropic\n   - Trade-off: slower but more accurate\n\n3. Handle errors gracefully:\n   - `get_valid_models()` can raise exceptions\n   - Log errors and return empty list on failure\n   - Maintain current error handling pattern\n\n## Files to Modify\n\n- `mcp_config_converter/llm/client.py` - Replace `get_available_models()` method implementation\n\n## Testing Considerations\n\n- Test with various providers (openai, anthropic, gemini, ollama, etc.)\n- Verify model filtering by provider works correctly\n- Test with and without `check_provider_endpoint=True`\n- Ensure error handling works when provider endpoints are unavailable\n- Test with environment variables set for different providers","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-04T03:44:21.1073251+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T03:45:18Z"}
{"id":"mcp-zqv","title":"Enable disk caching for LiteLLM completion calls","description":"## Context\n\nThe project already has `litellm[caching]\u003e=1.59.7` dependency installed in pyproject.toml (line 15), which provides disk caching capabilities. However, caching is not currently enabled or configured in LiteLLMClient class.\n\n## Problem\n\nCaching can significantly reduce costs and improve performance by:\n- Avoiding repeated API calls for identical prompts\n- Reducing latency for repeated operations\n- Lowering token usage and costs during development/testing\n\n## Implementation Details\n\nAccording to LiteLLM documentation (https://docs.litellm.ai/docs/caching/all_caches), disk caching is enabled by:\n\n1. Import Cache class:\n   ```python\n   from litellm.caching.caching import Cache\n   ```\n\n2. Initialize cache globally or per-client:\n   ```python\n   litellm.cache = Cache(type=\"disk\", disk_cache_dir=None)\n   # or using context manager\n   litellm.enable_cache(type=\"disk\")\n   ```\n\n3. Pass caching=True to completion calls:\n   ```python\n   response = completion(model=\"...\", messages=[...], caching=True)\n   ```\n\n## Requirements\n\n1. Modify `LiteLLMClient.__init__()` to:\n   - Add optional parameters: `enable_cache` (bool, default=False), `cache_type` (str, default=\"disk\"), `cache_dir` (str|None)\n   - Initialize litellm cache if enable_cache is True\n   - Support environment variable `MCP_CONFIG_CONF_LLM_CACHE_ENABLED` for default behavior\n\n2. Modify `LiteLLMClient.generate()` method:\n   - Pass `caching=True` to completion() calls when cache is enabled\n   - Consider adding cache control parameters (no-cache, no-store, ttl, s-maxage) as kwargs\n\n3. Add to constants.py:\n   - Environment variable constant: `MCP_CONFIG_CONF_LLM_CACHE_ENABLED`\n\n4. Add to CLI arguments (optional):\n   - Consider adding --enable-cache flag to relevant commands\n\n## Files to Modify\n\n- `mcp_config_converter/llm/client.py` - Main implementation in LiteLLMClient class\n- `mcp_config_converter/cli/constants.py` - Add cache environment variable constant\n- `mcp_config_converter/cli/arguments.py` - Optional: add CLI arguments for cache control\n\n## Testing Considerations\n\n- Test cache hit behavior with identical prompts\n- Test cache miss behavior with different prompts\n- Verify cache files are created in expected location\n- Test cache disable functionality\n- Ensure backward compatibility when cache is not enabled","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-04T03:44:21.0759305+01:00","created_by":"Jan Reimes","updated_at":"2026-01-04T04:07:31.1884864+01:00"}
